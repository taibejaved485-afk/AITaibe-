import { GoogleGenAI } from "@google/genai";
import { EditRequestParams } from "../types";

// Initialize the client
// API key must be provided via process.env.API_KEY
const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

const MODEL_NAME = 'gemini-2.5-flash-image';

/**
 * Edits an image based on a text prompt using Gemini 2.5 Flash Image.
 */
export const editImageWithGemini = async ({
  imageBase64,
  prompt,
  preserveFaces
}: EditRequestParams): Promise<string> => {
  try {
    // Clean base64 string if it contains metadata prefix
    const cleanBase64 = imageBase64.replace(/^data:image\/(png|jpeg|jpg|webp);base64,/, '');

    // Construct the prompt. We explicitly ask to preserve faces if requested.
    let finalPrompt = prompt;
    if (preserveFaces) {
      finalPrompt += " IMPORTANT: Preserve the identity and facial features of any people in the image exactly as they are. Do not distort faces.";
    }

    const response = await ai.models.generateContent({
      model: MODEL_NAME,
      contents: {
        parts: [
          {
            text: finalPrompt,
          },
          {
            inlineData: {
              mimeType: 'image/jpeg', // Standardize on JPEG for transmission if source allows
              data: cleanBase64,
            },
          },
        ],
      },
      // Gemini 2.5 Flash Image does not support responseMimeType or schemas
    });

    // Parse response for image data
    if (response.candidates && response.candidates[0] && response.candidates[0].content && response.candidates[0].content.parts) {
      for (const part of response.candidates[0].content.parts) {
        if (part.inlineData && part.inlineData.data) {
          return `data:image/png;base64,${part.inlineData.data}`;
        }
      }
    }

    throw new Error("No image generated by the model.");

  } catch (error) {
    console.error("Gemini Edit Error:", error);
    throw error;
  }
};

/**
 * Generates a new image using multiple input images (Reference + Targets).
 */
export const generateMultiImageTransform = async (
  imagesBase64: string[],
  prompt: string
): Promise<string> => {
  try {
    // Clean all base64 strings
    const cleanedImages = imagesBase64.map(img => 
      img.replace(/^data:image\/(png|jpeg|jpg|webp);base64,/, '')
    );

    // Construct content parts
    // Part 1: Text Prompt
    // Part 2...N: Images
    const parts: any[] = [
      {
        text: `${prompt} 
        
        INSTRUCTIONS:
        - The FIRST image provided is the REFERENCE image (Style/Character).
        - The SECOND image (and subsequent ones) is the TARGET content.
        - Transform the TARGET image to match the style, aesthetic, or character of the REFERENCE image.
        - High fidelity output.`
      }
    ];

    cleanedImages.forEach(imgData => {
      parts.push({
        inlineData: {
          mimeType: 'image/jpeg',
          data: imgData
        }
      });
    });

    const response = await ai.models.generateContent({
      model: MODEL_NAME,
      contents: { parts },
    });

    // Parse response
    if (response.candidates && response.candidates[0] && response.candidates[0].content && response.candidates[0].content.parts) {
      for (const part of response.candidates[0].content.parts) {
        if (part.inlineData && part.inlineData.data) {
          return `data:image/png;base64,${part.inlineData.data}`;
        }
      }
    }

    throw new Error("No image generated by the model.");

  } catch (error) {
    console.error("Gemini Multi-Image Error:", error);
    throw error;
  }
};
